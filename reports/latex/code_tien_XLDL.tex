
\section{Yêu cầu đồ án}
Với đề tài này, các bạn hãy tìm hiểu và cài đặt các API xử lý văn bản tiếng Việt (tiền xử lý, chuẩn hóa encoding, dấu câu, dấu thanh, loại bỏ các ký tự đặc biệt/dư thừa/mã HTML..., tách câu). API có thể thực hiện riêng rẽ hoặc đồng thời nhiều thao tác cùng lúc.

\section{Nội dung thực hiện}
{\bf Loại bỏ nhiễu và dữ liệu không cần thiết:} Văn bản thô thường chứa nhiều ký tự không cần thiết như dấu câu, ký tự đặc biệt, số, hoặc khoảng trắng thừa, mã html. Loại bỏ các thành phần này giúp giảm nhiễu trong dữ liệu và tập trung vào các thông tin có ý nghĩa.\\
{\bf Chuẩn hóa văn bản:} 
\begin{itemize}
    \item Giảm thiểu sự phân biệt chữ hoa và chữ thường: chuyển hết về chữ thường
    \item Chuẩn hóa encoding: Thay thế cách gõ Unicode tổ hợp bằng cách gõ của Unicode dựng sẵn.
    \item Chuẩn hoá dấu câu, dấu thanh
\end{itemize}
{\bf Tách từ:} Tiếng Việt là ngôn ngữ đa âm tiết, việc tách từ giúp nhận diện rõ ràng các từ trong câu. Điều này rất quan trọng vì nhiều mô hình NLP yêu cầu dữ liệu đầu vào là các từ hoặc cụm từ. \\
{\bf Tách câu:} Tách câu là một bước tiền xử lý quan trọng trong việc chuẩn bị dữ liệu cho nhiều tác vụ NLP khác nhau. Dữ liệu được tách câu sẽ dễ dàng hơn để gán nhãn, huấn luyện mô hình, và đánh giá kết quả\\
{\bf Loại bỏ từ dừng (StopWords):} Loại bỏ các từ dừng không mang nhiều ý nghĩa như "là", "và", "nhưng", "với",...

\subsection{Code các hàm Tiền xử lý dữ liệu}
File name: \textbf{ pre\_processing.py} là file code bao gồm toàn bộ các hàm dùng để tiền xử lý dữ liệu ngôn ngữ tiếng Việt.

\subsubsection{Loại bỏ các mã HTML ra khỏi văn bản.}

\begin{verbatim}
from bs4 import BeautifulSoup

def remove_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()
\end{verbatim}
    

\subsubsection{Loại bỏ tất cả các ký tự không phải là chữ cái hoặc số}
\begin{verbatim}
import re
def remove_special_characters(text):
    # Loại bỏ tất cả các ký tự không phải là chữ cái hoặc số
    return re.sub(r'[^a-zA-Z0-9\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđĐÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴÈÉẸẺẼÊỀẾỆỂỄÌÍỊỈĨÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠÙÚỤỦŨƯỪỨỰỬỮỲÝỴỶỸ.!?]', ' ', text)
\end{verbatim}

\subsubsection{Hàm chuẩn hóa về kiểu chữ thường}

\begin{verbatim}
def to_lowercase(text):
    return text.lower()
\end{verbatim}

\subsubsection{Hàm loại bỏ khoảng trắng thừa}
\begin{verbatim}
import re

def remove_extra_whitespace(text):
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace(" .", ".")
    text = text.replace(" !", "!")
    text = text.replace(" ?", "?")
    text = text.replace(" ,", ",")
    return text
\end{verbatim}

\subsubsection{Hàm chuẩn hoá dấu thanh}

\begin{verbatim}
def text_normalize_f(text):
    text = text_normalize(text)
    text = remove_extra_whitespace(text)
    return text
\end{verbatim}
\subsubsection{Hàm dùng để tách từ}

\begin{verbatim}
def pyvi_word_tokenize(text):
    return ViTokenizer.tokenize(text)
\end{verbatim}

\subsubsection{Hàm chuẩn hoá unicode}


\begin{verbatim}
# Chuẩn hoá unicode
def chuan_hoa_unicode(text):
    text = unicodedata.normalize('NFC', text)
    return text
\end{verbatim}
\subsubsection{Hàm loại bỏ từ dừng}

\begin{verbatim}
def remove_stop_words(text):
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)
\end{verbatim}
\subsubsection{Hàm chuẩn hoá dấu câu}
\begin{verbatim}
def normalize_punctuation(text):
    # Loại bỏ khoảng trắng thừa trước dấu câu
    text = re.sub(r'\s+([,.!?])', r'\1', text)
    # Thêm khoảng trắng sau dấu câu nếu chưa có
    text = re.sub(r'([,.!?])([^\s])', r'\1 \2', text)
    # Loại bỏ khoảng trắng thừa
    text = re.sub(r'\s+', ' ', text).strip()
    # Loại khoảng trắng thừa
    text = remove_extra_whitespace(text)
    return text
\end{verbatim}

\subsubsection{Hàm tách câu}


\begin{verbatim}
def sentence_tokenize(text):
    text = sent_tokenize(text)
    text_new_sent = []
    for _text in text:
        text_new = re.sub(
            r'[^a-zA-Z0-9\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđĐÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴÈÉẸẺẼÊỀẾỆỂỄÌÍỊỈĨÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠÙÚỤỦŨƯỪỨỰỬỮỲÝỴỶỸ_]', ' ', _text)
        text_new = re.sub(r'\s+', ' ', text_new).strip()
        
        if len(text_new) == 0:
            continue
        if text_new[-1] == " ":
            text_new = text_new[:-1]
        text_new_sent.append(text_new)

    return text_new_sent
\end{verbatim}

\subsubsection{Hàm tổng hợp để tiền xử lý văn bản}


\begin{verbatim}
def preprocess_text(text,
                    clean_html=True,
                    clean_special_char=True,
                    clean_extra_whitespace=True,
                    chuan_hoa_unicode_action=True,
                    chuan_hoa_chu_thuong= True,
                    chuan_hoa_dau_thanh=True,
                    chuan_hoa_dau_cau= True,
                    split_word_pyvi=False,
                    split_word_vietokenizer=True,
                    split_sent=True,
                    remove_sw=True
                    ):
    if clean_html:
        text = remove_html(text)
    if clean_special_char:
        text = remove_special_characters(text)
    if clean_extra_whitespace:
        # Loại khoảng trắng thừa
        text = remove_extra_whitespace(text)
    # Chuẩn hoá dữ liệu
    if chuan_hoa_unicode_action:
        text = chuan_hoa_unicode(text)
    if chuan_hoa_chu_thuong:
        text = to_lowercase(text)
    if chuan_hoa_dau_thanh:
        text = text_normalize_f(text)
    if chuan_hoa_dau_cau:
        text = normalize_punctuation(text)
    if split_word_pyvi:
        text = pyvi_word_tokenize(text)
        # Chuẩn hoá dấu câu
        text = normalize_punctuation(text)
        # Loại khoảng trắng thừa
        text = remove_extra_whitespace(text)
    elif split_word_vietokenizer:
        text = word_tokenize(text)
    if remove_sw:
        text = remove_stop_words(text=text)
    # Tách câu
    if split_sent:
        text = sentence_tokenize(text)
        return text
    elif clean_special_char:
        # Loại bỏ kí tự đặc biệt
        text = re.sub(
            r'[^a-zA-Z0-9\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđĐÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴÈÉẸẺẼÊỀẾỆỂỄÌÍỊỈĨÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠÙÚỤỦŨƯỪỨỰỬỮỲÝỴỶỸ_]', ' ', text)
        # Loại khoảng trắng thừa
        text = remove_extra_whitespace(text)

    return [text]
\end{verbatim}



